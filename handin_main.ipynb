{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3f3abe",
   "metadata": {},
   "source": [
    "# Exploring Wikipedia's Network Structure in a Temporal Fashion\n",
    "Authors: Felix Borum Burmester & Christian Vestergaard Djurhuus.\n",
    "\n",
    "In this notebook, we will explore the temporal development of Marvel- and DC-Comics. We have used the Wikipedia Revisions api to scrape the Wikipedia pages of 2303 superheroes since the dawn of Wikipedia. We have used these pages to create a directed, temporal graph object that will allow us to understand the development of revisions.\n",
    "\n",
    "We have also webscraped meta data from lists of female, black, latino, and asian superheroes and villains. With this meta data we were able to appoint node attributes to our graph with the help of document similarity (since different websites may call superheroes by different versions of their names).\n",
    "\n",
    "<img src=\"G_plot_final.gif\" width=\"750\" align=\"center\">\n",
    "\n",
    "The red nodes here are Marvel characters, and the blue are DC characters!\n",
    "\n",
    "This investigative notebook will attempt to discover characteristics about the network as well as investigate to its evolution. This will be done by exploring basic statistics of the network and how they evolve, how communities evolve and finally how communities change with time in the network.\n",
    "\n",
    "**TODO: Extend intro to mention purpose and hypotheses**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d041c0",
   "metadata": {},
   "source": [
    "## Revision Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be3d69",
   "metadata": {},
   "source": [
    "Wikipedia is an online encyclopedia that is created and altered by a community of volunteers from all over the world. Hence, the content of a Wikipedia page may be changed through revisions conducted by the volunteers. The revisions of a given Wikipedia page are timestamped, stored and made publicly available by the Wikipedia Revisions API. This allows individuals to analyse how the content of Wikipedia changes with time.\n",
    "\n",
    "The content of a Wikipedia page often references other Wikipedia pages using hyperlinks, which allow us to represent Wikipedia as a directed network $G=(\\mathcal{N}, \\mathcal{V})$, where the pages correspond to the nodes $\\mathcal{N}$ and hyperlinks between Wikipedia pages corresponds to the edges/links $\\mathcal{V}$.\n",
    "\n",
    "In our specific case, we want to investigate and understand a minor subset of the Wikipedia network focusing on how the superhero pages of Wikipedia reference each other. However, instead of analysing the static Wikipedia network, we will use the Wikipedia Revisions API to gather the different versions of the superhero pages from 2001 to 2022. We have decided to discretise the time interval by querying the last revision of a superhero page each year. This allows us to represent the superhero Wikipedia link network as a temporal network using a sequence of static networks corresponding to each year. The usage of a temporal network instead of a static network will allow us to investigate how the network has evolved from its origin to this date.\n",
    "\n",
    "The wiring of the temporal network is changed by the volunteers' revisions, e.g., when they either add or remove a hyperlink to another Wikipedia page. Therefore, the first part of this notebook will investigate the number of revisions as a function of time to get an initial impression of the revision distribution and how frequent the structure of the temporal network may be altered:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e03e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = 'superheroes/'\n",
    "df_revisions = pd.DataFrame()\n",
    "df_revisions\n",
    "\n",
    "total_counts = dict.fromkeys([2000 + n for n in range(1, 2022-2001 + 2)], 0)\n",
    "\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    try:\n",
    "        with open(path_folder + row.CharacterName + '/' + 'number_of_revisions.pkl', 'rb') as handle:\n",
    "            revision_counts = pickle.load(handle)\n",
    "            for key, item in revision_counts.items():\n",
    "                total_counts[key] += item\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print(f'An error occured: {e}')\n",
    "\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    try:\n",
    "        with open(path_folder + row.CharacterName + '/' + 'number_of_revisions.pkl', 'rb') as handle:\n",
    "            revision_counts = pickle.load(handle)\n",
    "        temp_rv_df = pd.DataFrame.from_dict({'counts':revision_counts.values(), 'Year':revision_counts.keys(), 'Character':[row.CharacterName]*len(list(revision_counts.keys()))})\n",
    "        df_revisions = df_revisions.append(temp_rv_df, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        #print(f'An error occured: {e}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b24853",
   "metadata": {},
   "source": [
    "The following figure illustrates the total number of revisions within a specific year for the superheroes wikipedia pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf000cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "ax.plot([str(item) for item in list(total_counts.keys())], list(total_counts.values()), '-o', c='darkblue')\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"counts\")\n",
    "ax.set_title(\"Total number of revisions\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dad73a",
   "metadata": {},
   "source": [
    "It seems that revisions were at a peak during the big boom in Wikipedia (2004-2007). After this, the revisions slowly decline. It seems, however, that during the movies of the MCU and DCU, the revisions reached a new peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38301e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas.api.types import CategoricalDtype\n",
    "\n",
    "#cat_character = CategoricalDtype(df_revisions.Character.unique())\n",
    "#df_revisions['Character'] = df_revisions['Character'].astype(cat_character)\n",
    "metadata = pd.read_csv('metadataproject.csv')\n",
    "temp = pd.merge(df_revisions, metadata, left_on='Character', right_on='CharacterName') \n",
    "#Creating female and male dataframe\n",
    "df_male = temp[temp['man']==1]\n",
    "df_women = temp[temp['woman']==1]\n",
    "\n",
    "counts_women = df_women.groupby(['Year']).sum().counts\n",
    "counts_men =  df_male.groupby(['Year']).sum().counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "ax.plot(counts_men.index.astype(str), counts_men.values, '-o', c='darkblue', label=\"Superheroes\")\n",
    "ax.plot(counts_women.index.astype(str), counts_women.values, '-o', c='firebrick', label=\"Superheroines\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "ax.set_title(\"Number of revisions each snapshot\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef50f14",
   "metadata": {},
   "source": [
    "There seems to be almost the same trend with the exception that 2017 seems to be a small peak for superheroines and a valley for superheroes. Overall, however, there are more revisions on superheroes than on superheroines during the entire period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff74e08",
   "metadata": {},
   "source": [
    "## Exploration of Basic Stats in Network\n",
    "### Node Analysis\n",
    "Let's view the number of nodes with each label by time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "def setup_mpl():\n",
    "    mpl.rcParams['font.family'] = \"Times New Roman\"\n",
    "    return\n",
    "setup_mpl()\n",
    "\n",
    "#import data\n",
    "meta_df = pd.read_csv('metadataproject.csv')\n",
    "edgelist = pd.read_csv('corrected_edgelist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bc35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b464f2",
   "metadata": {},
   "source": [
    "As you can see, there is not meta data for all nodes; Achebe has no gender, race or alignment in our data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03195c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d1fdc7",
   "metadata": {},
   "source": [
    "The edgelist has timestamps for when the link is valid. This is used to later create time-specific snapshots of the graph!\n",
    "\n",
    "Now we create a node list and merge it with our meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e204486",
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcenodes = pd.DataFrame(np.array(list(edgelist.groupby(['source','timestamp']).groups.keys())), columns = ['node','timestamp'])\n",
    "targetnodes = pd.DataFrame(np.array(list(edgelist.groupby(['target','timestamp']).groups.keys())), columns = ['node','timestamp'])\n",
    "#keep unique nodes and timestamps!\n",
    "nodes =  pd.concat([sourcenodes,targetnodes]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "temp = meta_df.rename(columns={'WikiLink':'node'})\n",
    "nodes = pd.merge(nodes, temp, on=\"node\")\n",
    "nodes = nodes.drop(columns=['Unnamed: 0'])\n",
    "nodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aeffb1",
   "metadata": {},
   "source": [
    "We need some pd.series to plot, so we use groupby and its features to get them and create three plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb84380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get plotable data for marvel and dc\n",
    "total_nodes_by_year = nodes.groupby('timestamp').size()\n",
    "temp = nodes.groupby(['universe'])\n",
    "dc_nodes_by_year, marvel_nodes_by_year = [temp.get_group(x) for x in temp.groups]\n",
    "dc_nodes_by_year = dc_nodes_by_year.groupby('timestamp').size()\n",
    "marvel_nodes_by_year = marvel_nodes_by_year.groupby('timestamp').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b962b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "ax.plot(dc_nodes_by_year.index, dc_nodes_by_year.values, '-o', c='lightblue', label='DC')\n",
    "ax.plot(marvel_nodes_by_year.index, marvel_nodes_by_year.values, '-o', c='firebrick', label='Marvel')\n",
    "ax.plot(total_nodes_by_year.index, total_nodes_by_year.values, '-o', c='darkblue',label='Total')\n",
    "ax.set_title(\"Number of nodes for each snapshot (Universe)\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b09469f",
   "metadata": {},
   "source": [
    "We see here that there is a surge in number of nodes from 2003 to 2007 where it looks like it is more stable with a slight upwards trend. This might overshadow further analysis of the graph. One might consider making a cut and only view the data from 2007 to 2022.\n",
    "\n",
    "Let's view some other features in the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158bb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get plotable data for sex, race and alignment!\n",
    "temp = nodes.groupby(['man'])\n",
    "man_nodes_by_year = [temp.get_group(x) for x in temp.groups][1].groupby('timestamp').size()\n",
    "temp = nodes.groupby(['woman'])\n",
    "woman_nodes_by_year = [temp.get_group(x) for x in temp.groups][1].groupby('timestamp').size()\n",
    "temp = nodes.groupby(['asian'])\n",
    "asian_nodes_by_year = [temp.get_group(x) for x in temp.groups][1].groupby('timestamp').size()\n",
    "temp = nodes.groupby(['latino'])\n",
    "latino_nodes_by_year = [temp.get_group(x) for x in temp.groups][1].groupby('timestamp').size()\n",
    "temp = nodes.groupby(['black'])\n",
    "black_nodes_by_year = [temp.get_group(x) for x in temp.groups][1].groupby('timestamp').size()\n",
    "temp = nodes.groupby(['good'])\n",
    "good_nodes_by_year = [temp.get_group(x) for x in temp.groups][1].groupby('timestamp').size()\n",
    "temp = nodes.groupby(['bad'])\n",
    "bad_nodes_by_year = [temp.get_group(x) for x in temp.groups][1].groupby('timestamp').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "ax.plot(man_nodes_by_year.index, man_nodes_by_year.values, '-o', c='lightblue', label='Men')\n",
    "ax.plot(woman_nodes_by_year.index, woman_nodes_by_year.values, '-o', c='firebrick', label='Women')\n",
    "ax.set_title(\"Number of nodes for each snapshot (Sex)\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d28e8",
   "metadata": {},
   "source": [
    "The same surge-trend seems to be present here as well. There is no indication from this plot that wokeism have created more superheroine wikipedia articles at least. Let's see the same plot with everything pre 2007 cut off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "ax.plot(man_nodes_by_year.index, man_nodes_by_year.values, '-o', c='lightblue', label='Men')\n",
    "ax.plot(woman_nodes_by_year.index, woman_nodes_by_year.values, '-o', c='firebrick', label='Women')\n",
    "ax.set_title(\"Number of nodes for each snapshot (Sex)\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.xlim('2007','2022')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d1e6e",
   "metadata": {},
   "source": [
    "Our verdict does not change.. But what about race?\n",
    "\n",
    "It was not possible to find any list of caucasian superheroes or villains. You might be able to conclude that the rest are white, but as some superheroes or villains are aliens, it would not be correct to assume them to be white. \"Why not find a list of extraterestial superheroes?\" Well, Thor, Loki and Odin are extraterestial but they are also caucasian. We found it best not to mess with this as the data would be highly inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "ax.plot(latino_nodes_by_year.index, latino_nodes_by_year.values, '-o', c='firebrick', label='Latino')\n",
    "ax.plot(asian_nodes_by_year.index, asian_nodes_by_year.values, '-o', c='darkblue',label='Asian')\n",
    "ax.plot(black_nodes_by_year.index, black_nodes_by_year.values, '-o', c='lightblue',label='Black')\n",
    "ax.set_title(\"Number of nodes for each snapshot (Race)\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6145580",
   "metadata": {},
   "source": [
    "Here we see some interesting trends. The number of Wiki articles for Latinos and Asians seem to steadily increase while Black superhero articles seem to decline. Perhaps this is worth further exploration in the project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "ax.plot(good_nodes_by_year.index, good_nodes_by_year.values, '-o', c='lightblue', label='Good Alignment')\n",
    "ax.plot(bad_nodes_by_year.index, bad_nodes_by_year.values, '-o', c='firebrick',label='Bad Alignment')\n",
    "ax.set_title(\"Number of nodes for each snapshot (Alignment)\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4672b",
   "metadata": {},
   "source": [
    "It makes sense that there would be about the same number of superheros as super villains (and that the trends would follow each other closely).\n",
    "\n",
    "All plots of nodes seem to follow the general Wikipedia trend ([with the massive surge in 2003-2006](https://en.wikipedia.org/wiki/Wikipedia:Statistics)). It is still interesting that there have been a lot more pages created on male superheroes as compared to female superheroes in general. This could indicate that there are more male superheroes to create articles about and that the ones creating articles are more interested in them (perhaps because they are males themselves)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99c9d3",
   "metadata": {},
   "source": [
    "### Edge Analysis\n",
    "In the following, the evolution of number of edges in the temporal Wikipedia-link superhero network will be investigated. To evaluate the evolution, we have decided to represent the temporal network as a sequence of static networks corresponding to a yearly snapshot of the original temporal network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed15fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import networkx as nx\n",
    "import netwulf as nw \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbaba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marvel = pd.read_csv(\"files/marvel.csv\", index_col=0, encoding='utf8')\n",
    "df_marvel = df_marvel[df_marvel[\"WikiLink\"].notna()]\n",
    "df_dc = pd.read_csv(\"files/dc.csv\", index_col=0, encoding='utf8')\n",
    "df_dc = df_dc[df_dc[\"WikiLink\"].notna()]\n",
    "df_marvel['universe'] = 'Marvel'\n",
    "df_dc['universe'] = 'DC'\n",
    "df = pd.concat([df_marvel, df_dc], ignore_index=True, axis=0)\n",
    "edgelist = pd.read_csv(\"corrected_edgelist.csv\")\n",
    "edgelist = edgelist.drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768551ff",
   "metadata": {},
   "source": [
    "The following figure illustrates the total number of edges at each yearly snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f860ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = edgelist.groupby(\"timestamp\").size()\n",
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "ax.plot(counts.index.astype(str), counts.values, '-o', c='darkblue')\n",
    "ax.set_title(\"Number of links for each snapshot\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "#ax.set_yscale(\"log\")\n",
    "ax.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b25d3",
   "metadata": {},
   "source": [
    "As seen in the above figure, the total number of links each year appears to demonstrate a nearly exponential growth within the initial period of 2001 to 2006, followed by a somewhat linear growth which may begin to plateau in the year 2022. Furthermore, the exponential trend present between 2001 and 2006 aligns with the trend of the growth w.r.t the number of nodes presented in the figures above. The linear increase in the number of edges after 2006 deviates from the trend associated with the growth of nodes. The number of nodes appears to stagnate after 2006, which intuitively makes sense given that set of superheroes used to generate the network was finite and consisted of a little more than 2000 superheroes, while the edges among Wikipedia is less constrained and only strictly restricted by the total number of possible edges in a directed network:\n",
    "$$N*(N-1)\\approx 2000*(2000-1) \\approx 4,000,000$$\n",
    "\n",
    "However, it is important to mention that the number of links in the observed network is significantly less than the possible number of links. This indicates the sparsity of the network, which is also a common characteristic associated with real networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccacdf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.merge(edgelist, df, left_on='source', right_on='WikiLink')\n",
    "temp = temp.rename(columns={\"universe\":\"source universe\"})\n",
    "temp = temp.drop(columns=['CharacterName', 'WikiLink'])\n",
    "temp = pd.merge(temp, df, left_on='target', right_on='WikiLink')\n",
    "temp = temp.rename(columns={\"universe\":\"target universe\"})\n",
    "temp = temp.drop(columns=['CharacterName', 'WikiLink'])\n",
    "expanded_edgelist = temp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c3795",
   "metadata": {},
   "source": [
    "Furthermore, to investigate how many links DC and Marvel have respectively. The number of edges within each of the two comics universes was illustrated along with the number of edges across the two. Hence, the following figure demonstrates the intra and inter universe links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9131d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = expanded_edgelist.groupby([\"timestamp\",\"source universe\", \"target universe\"]).size().reset_index(name=\"counts\")\n",
    "dc_links = []\n",
    "marvel_links = []\n",
    "marvel_dc_links = []\n",
    "dc_marvel_links = []\n",
    "for idx, row in count_df.iterrows():\n",
    "    if row[\"source universe\"] == \"DC\" and row[\"target universe\"] == \"DC\":\n",
    "        dc_links.append(row.counts)\n",
    "    elif row[\"source universe\"] == \"Marvel\" and row[\"target universe\"] == \"Marvel\":\n",
    "        marvel_links.append(row.counts)\n",
    "    elif row[\"source universe\"] == \"Marvel\" and row[\"target universe\"] == \"DC\":\n",
    "        marvel_dc_links.append(row.counts)\n",
    "    else:\n",
    "        dc_marvel_links.append(row.counts)\n",
    "marvel_dc_links.insert(0, 0)#No inter connected at year 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4679d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "\n",
    "ax.plot(count_df.timestamp.astype(str).unique(), marvel_links, '-o', c='firebrick',label=\"Marvel\")\n",
    "ax.plot(count_df.timestamp.astype(str).unique(), dc_links, '-o', c='lightblue',label=\"DC\")\n",
    "ax.plot(count_df.timestamp.astype(str).unique(), marvel_dc_links, '-o', c='darkblue',label=\"Marvel & DC\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Number of links at each snapshot \\n within and across universe communities\")\n",
    "ax.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f94c4f1",
   "metadata": {},
   "source": [
    "The illustration of the number of links within and across the two universes demonstrates that the Marvel universe contains a significantly greater number of links compared with the DC universe. Interestingly, the number of links between the two universes seems to be almost negligible throughout the entire timespan, which indicates that two universes coexist within the superhero network as almost two disconnected communities compared with the number of edges within the two universes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38980021",
   "metadata": {},
   "source": [
    "### Degree Distribution Analysis\n",
    "Let's first view the average degree of nodes in the graph at each timestep to determine if this follows the same trend as the edge analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create edgelist for each year\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "edgelist_divided = {}\n",
    "groups = edgelist.groupby('timestamp').groups\n",
    "for key, index in groups.items():\n",
    "    edgelist_divided[key] = edgelist.loc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b82bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will load in the graph from a list of gml files\n",
    "#as setting node attributes takes a while \n",
    "#(and does not look pretty)\n",
    "graphs = {}\n",
    "for key in np.arange(2001,2023,1):\n",
    "    graphs[key] = nx.read_gml(f'graphs/DG_{key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee43f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate degrees\n",
    "avg_degrees = {}\n",
    "for key in graphs.keys():\n",
    "    avg_degrees[key] = sum([graphs[key].in_degree(x) for x in graphs[key].nodes()])/graphs[key].number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c7563",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "ax.plot(list(avg_degrees.keys()), list(avg_degrees.values()), '-o', c='lightblue', label='Average Degree')\n",
    "ax.set_title(\"Average Degree\")\n",
    "ax.set_ylabel(\"Degree\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ebb86",
   "metadata": {},
   "source": [
    "It follows the exact same trend as the number of edges as expected because the average degree of a directed network is defined as the total number of links divided by the total number of nodes [3]:\n",
    "$$\\langle k\\rangle=\\frac{L}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30859d56",
   "metadata": {},
   "source": [
    "#### Static Analysis of Degree Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa5aee2",
   "metadata": {},
   "source": [
    "To attempt to understand the characteristics of the temporal network, we analyse the degree distribution. Initially, the analysis of the degree distribution of the temporal network will be restricted to the final time step $t_{N}$ and treated as a static network. Subsequently, we will analyse the degree distribution at each timestep to investigate if any changes emerge in the network characteristics as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75171e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(expanded_edgelist.timestamp.unique())\n",
    "tN = years[-1]\n",
    "edgelist_tN = expanded_edgelist[expanded_edgelist.timestamp==tN]\n",
    "attrs_tN = {row.source: {'group': row['source universe']} for idx, row in edgelist_tN.iterrows()}\n",
    "G_tN = nx.from_pandas_edgelist(edgelist_tN, create_using = nx.DiGraph)\n",
    "#Adding attributes\n",
    "nx.set_node_attributes(G_tN, attrs_tN)\n",
    "#Fixing issue with nodes missing universe data\n",
    "missing_attrs = {row.target: {'group': row['target universe']}for idx, row in edgelist_tN[edgelist_tN['target'].isin([u for u,v in G_tN.nodes(data=True) if not v])].iterrows()}\n",
    "nx.set_node_attributes(G_tN, missing_attrs)\n",
    "#Removing singleton nodes\n",
    "G_tN.remove_nodes_from(list(nx.isolates(G_tN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1a715",
   "metadata": {},
   "source": [
    "The following figures illustrate the in-degree and out-degree distribution using a linear binning and linear x- and y-scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degrees = [y for (x,y) in G_tN.in_degree]\n",
    "out_degrees = [y for (x,y) in G_tN.out_degree]\n",
    "\n",
    "bins = np.arange(min(in_degrees), max(in_degrees), 1)\n",
    "hist, edges = np.histogram(in_degrees, bins=bins)\n",
    "x = (edges[1:] + edges[:-1])/2\n",
    "width = bins[1]-bins[0]\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,6), dpi=100)\n",
    "ax1.bar(x, hist, width=width*0.91)\n",
    "ax1.set_ylabel(\"counts\")\n",
    "ax1.set_xlabel(\"k\")\n",
    "ax1.set_title(\"In degree Distribution of tN\")\n",
    "\n",
    "bins = np.arange(min(out_degrees), max(out_degrees), 1)\n",
    "hist, edges = np.histogram(out_degrees, bins=bins)\n",
    "x = (edges[1:] + edges[:-1])/2\n",
    "width = bins[1]-bins[0]\n",
    "ax2.bar(x, hist, width=width*0.90)\n",
    "ax2.set_ylabel(\"counts\")\n",
    "ax2.set_xlabel(\"k\")\n",
    "ax2.set_title(\"Out degree Distribution of tN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f804a",
   "metadata": {},
   "source": [
    "The main finding in the above figures is the apparent difference between the characteristics of the in-degree and out-degree distributions. The out-degree distribution resembles a Poisson distribution, which typically is associated with the degree distribution of a random network. However, the in-degree distribution appears to follow a heavy-tailed power-law distribution often associated with scale-free networks. To further investigate if the two degree distributions follow the distributions proposed above, the in-degree and out-degree distributions are illustrated using a logarithmic binning and a logarithmic x- and y-scale. If the in-degree distribution follows a power-law distribution, the degree distribution should fall on a straight line when using a logarithmic scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea631c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degrees = [d for n, d in G_tN.in_degree()]\n",
    "min_in_deg =min(in_degrees)\n",
    "max_in_deg =max(in_degrees)\n",
    "min(in_degrees), max(in_degrees)\n",
    "\n",
    "#bins = np.arange(min_in_deg, max_in_deg, 1) #Linear binning\n",
    "bins = np.logspace(0, np.log10(max_in_deg), 30) # log binning\n",
    "in_hist, in_edges = np.histogram(in_degrees, bins=bins)\n",
    "in_x = (in_edges[1:]+in_edges[:-1])/2\n",
    "\n",
    "\n",
    "out_degrees = [d for n, d in G_tN.out_degree()]\n",
    "min_out_deg =min(out_degrees)\n",
    "max_out_deg =max(out_degrees)\n",
    "min(out_degrees), max(out_degrees)\n",
    "\n",
    "#bins = np.arange(min_out_deg, max_out_deg, 1) #Linear binning\n",
    "bins = np.logspace(0, np.log10(max_out_deg), 30) # log binning\n",
    "out_hist, out_edges = np.histogram(out_degrees, bins=bins)\n",
    "out_x = (out_edges[1:]+out_edges[:-1])/2\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,6), dpi=100)\n",
    "\n",
    "in_xx, in_yy = zip(*[(i,j) for (i,j) in zip(in_x, in_hist) if j > 0])\n",
    "ax1.plot(in_xx, in_yy, marker='.', linestyle='None')\n",
    "ax1.set_ylabel(\"counts\")\n",
    "ax1.set_xlabel(\"k\")\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax1.set_title(\"In-degree Distribution ($t_{N}$)\")\n",
    "\n",
    "out_xx, out_yy = zip(*[(i,j) for (i,j) in zip(out_x, out_hist) if j > 0])\n",
    "ax2.plot(out_xx, out_yy, marker='.', linestyle='None')\n",
    "ax2.set_ylabel(\"counts\")\n",
    "ax2.set_xlabel(\"k\")\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_title(\"Out-degree Distribution ($t_{N}$)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84869709",
   "metadata": {},
   "source": [
    "In-degree distributions of the network at $t_{N}$ appear to follow a scale-free network structure, while the out-degree distribution resembles the degree distribution of a random network. Intuitively, this finding makes sense given that the in-degree reflects the number of Wikipedia pages that reference this specific page, while the out-degree reflects how many other Wikipedia pages this given page references. Naturally, the out-degree will be more constrained than the in-degree, given that there is a naturally occurring limit to how many references one can gather on a single Wikipedia page. The in-degree is less constrained and somewhat reflects the popularity of a single page. A similar phenomenon can be seen in follower-based social media platforms e.g. Twitter, where it seems that the number of followers a person might have is almost unconstrained except for the number of users of the platform ([e.g. Justin Bieber having 113.8 million followers on Twitter](https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts)), while the number of individuals a person might follow is limited by the cognitive demand of keeping up with all the content these individuals produce. The fact that the In-degree distribution is heavy-tailed and resembles a power law implies that the network contains hubs, which are Wikipedia pages being referenced by other Wikipedia pages much more than average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4154d",
   "metadata": {},
   "source": [
    "#### Temporal Analysis of Degree Distributions\n",
    "Additionally, the in-degree and out-degree distributions are determined at each yearly snapshot of the temporal network to gain insight into how the degree distributions have changed with time. The degree distributions are plotted along side each other using a logarithmic binning and scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e71f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "import networkx as nx\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,6), dpi=100)\n",
    "\n",
    "years = sorted([x for x in expanded_edgelist.timestamp.unique()])\n",
    "n = len(years)\n",
    "#colors = pl.cm.plasma(np.linspace(0,1,n))\n",
    "colors = pl.cm.jet(np.linspace(0,1,n))\n",
    "for idx, year in enumerate(years):\n",
    "    edgelist_temp = expanded_edgelist[expanded_edgelist.timestamp == year]\n",
    "    G_temp = nx.from_pandas_edgelist(edgelist_temp, create_using = nx.DiGraph)\n",
    "\n",
    "    #Removing singleton nodes\n",
    "    G_temp.remove_nodes_from(list(nx.isolates(G_temp)))\n",
    "\n",
    "    in_degrees = [d for n, d in G_temp.in_degree()]\n",
    "    min_in_deg =min(in_degrees)\n",
    "    max_in_deg =max(in_degrees)\n",
    "    \n",
    "    #bins = np.arange(min_in_deg, max_in_deg, 1)\n",
    "    bins = np.logspace(0, np.log10(max_in_deg), 30) # log binning\n",
    "    in_hist, in_edges = np.histogram(in_degrees, bins=bins)\n",
    "    in_x = (in_edges[1:]+in_edges[:-1])/2\n",
    "\n",
    "\n",
    "    out_degrees = [d for n, d in G_temp.out_degree()]\n",
    "    min_out_deg =min(out_degrees)\n",
    "    max_out_deg =max(out_degrees)\n",
    "\n",
    "    #bins = np.arange(min_out_deg, max_out_deg, 1) #Linear binning\n",
    "    bins = np.logspace(0, np.log10(max_out_deg), 30) # log binning\n",
    "    out_hist, out_edges = np.histogram(out_degrees, bins=bins)\n",
    "    out_x = (out_edges[1:]+out_edges[:-1])/2\n",
    "    \n",
    "    in_xx, in_yy = zip(*[(i,j) for (i,j) in zip(in_x, in_hist) if j > 0])\n",
    "    ax1.plot(in_xx, in_yy, marker='.', c=colors[idx], label=str(year))\n",
    "    ax1.set_ylabel(\"counts\")\n",
    "    ax1.set_xlabel(\"k\")\n",
    "    ax1.set_xscale(\"log\")\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax1.set_title(\"Temporal in degree distribution\")\n",
    "\n",
    "    out_xx, out_yy = zip(*[(i,j) for (i,j) in zip(out_x, out_hist) if j > 0])\n",
    "    ax2.plot(out_xx, out_yy, marker='.', c=colors[idx], label=str(year))\n",
    "    ax2.set_ylabel(\"counts\")\n",
    "    ax2.set_xlabel(\"k\")\n",
    "    ax2.set_xscale(\"log\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.set_title(\"Temporal out degree distribution\")\n",
    "box = ax1.get_position()\n",
    "ax1.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "                 box.width, box.height * 0.9])\n",
    "\n",
    "# Put a legend below current axis\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e157ec2",
   "metadata": {},
   "source": [
    "The figures illustrate that the temporal in-degree distribution converges into a degree distribution that resembles a power-law distribution. The temporal out-degree distribution also demonstrates that it converges into a distribution similar to a Poisson distribution when time passes. Furthermore, the figures indicate that the stability in the degree distributions increases, given that the yearly difference in degree distributions appear to decrease with time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ac798",
   "metadata": {},
   "source": [
    "### Investigation of selected superheros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56fa16",
   "metadata": {},
   "source": [
    "#### Captain America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_folder + 'Captain America'+ '/' + 'number_of_revisions.pkl', 'rb') as handle:\n",
    "    cap_revisions = pickle.load(handle)\n",
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "ax.plot([str(item) for item in list(cap_revisions.keys())], list(cap_revisions.values()), '-o', c='darkblue')\n",
    "ax.grid()\n",
    "plt.axvline(x=\"2011\", linestyle='--',c='orange', label=\"The first avenger\")\n",
    "plt.axvline(x=\"2014\", linestyle='--',c='firebrick', label=\"The winter soldier\")\n",
    "plt.axvline(x=\"2016\", linestyle='--',c='darkgreen', label=\"Civil war\")\n",
    "ax.set_ylabel(\"counts\")\n",
    "ax.set_title(\"Captain America \\n Number of revisions\")\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac9dae",
   "metadata": {},
   "source": [
    "It is interesting to see a very popular Marvel character like Captain America. He has many revisions at all times and other than Civil war, it doesn't look like movie release dates revolving about him creates spikes unlike Black Panther below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c750d9",
   "metadata": {},
   "source": [
    "#### Black Panther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74911121",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_folder + 'Black Panther'+ '/' + 'number_of_revisions.pkl', 'rb') as handle:\n",
    "    black_panther_revisions = pickle.load(handle)\n",
    "fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "ax.plot([str(item) for item in list(cap_revisions.keys())], list(black_panther_revisions.values()), '-o', c='darkblue')\n",
    "ax.axvline(x=\"2018\", linestyle='--',c='firebrick', label=\"Black Panther (film)\")\n",
    "ax.axvline(x=\"2014\", linestyle='--', c='darkgreen', label=\"First announcement\")\n",
    "ax.axvline(x=\"2016\", linestyle='--', c='orange', label=\"First appearance in MCU\")\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"counts\")\n",
    "ax.set_title(\"Black Panther \\n Number of revisions\")\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3f879",
   "metadata": {},
   "source": [
    "Here we see that appearance in the MCU and his own movie really introduces spikes.\n",
    "\n",
    "In general, it seems that the introduction of less familiar characters introduces way more spikes in revisions as there is not a lot of stuff to write about untill they appear in movies or new comic books. Captain America has had his MCU story written a long time ago while Black Panther's is only getting written in later years.\n",
    "\n",
    "What are the last spikes? In 2020 Chadwick Boseman (the actor) died and in 2021 a new comic book issue of Black Panther is released which was quite hyped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b1d0f",
   "metadata": {},
   "source": [
    "# Does the Gender and Race of a Character Play a Role in the Network?\n",
    "To answer this question, we will use the general permutation test framework, where the gender and race properties of the nodes will be randomly shuffled. Hence, creating an ensemble of networks with random node properties, which subsequently will be compared with the observed network. Using the permutation test framework one can easily compute the p-value to determine if there is a significant difference between the test statistic of the permuted and the observed networks.\n",
    "\n",
    "The test statistic of interest in this investigation is the average node betweenness centrality w.r.t gender and race, respectively. The initial part will focus on gender followed by the race. Therefore, the question of interest is whether or not gender and race influence the centrality of a node in the network.\n",
    "\n",
    "To determine the significance of e.g. gender, the following steps will be carried out:\n",
    "1) Determine the average betweenness centrality of female and male characters, respectively. ($C_{female}$, $C_{male}$)\n",
    "<br >\n",
    "\n",
    "2) Conduct a thousand permutations of the nodes' gender property and calculate the average betweenness centrality for each permutation to obtain ($\\{C_{female}(R_{1}), C_{female}(R_{2}),..., C_{female}(R_{1000}) \\}$ & $\\{C_{male}(R_{1}), C_{male}(R_{2}),..., C_{male}(R_{1000}) \\}$)\n",
    "<br >\n",
    "\n",
    "3) Compute the p-value, which corresponds to the proportion of permuted test statistics that exceeds the observed test statistic.\n",
    "\n",
    "We have decided to only conduct the one-sided permutation test. However, in some cases, we conduct two one-sided permutation tests to test if the test statistic is significantly higher or lower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36713e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_map = {}\n",
    "for idx, row in meta_df.iterrows():  \n",
    "    if row['woman'] == 1:\n",
    "        gender_map[row.WikiLink] = \"Female\"\n",
    "    elif row['man'] == 1:\n",
    "        gender_map[row.WikiLink] = \"Male\"\n",
    "    else:\n",
    "        gender_map[row.WikiLink] = \"NaN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd6d0d",
   "metadata": {},
   "source": [
    "Initially, the permutation test will investigate if the gender of a superhero play any significant role in the network in 2022 w.r.t node centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "edgelist_temp = expanded_edgelist[expanded_edgelist.timestamp==2022]\n",
    "attrs_temp = {row.source: {'group': gender_map[row.source]} for idx, row in edgelist_temp.iterrows()}\n",
    "G_temp = nx.from_pandas_edgelist(edgelist_temp, create_using = nx.DiGraph)\n",
    "\n",
    "#Adding attributes\n",
    "nx.set_node_attributes(G_temp, attrs_temp)\n",
    "\n",
    "#Fixing issue with nodes missing universe data\n",
    "missing_attrs = {row.target: {'group': gender_map[row.target]}for idx, row in edgelist_temp[edgelist_temp['target'].isin([u for u,v in G_temp.nodes(data=True) if not v])].iterrows()}\n",
    "nx.set_node_attributes(G_temp, missing_attrs)\n",
    "\n",
    "\n",
    "#Removing singleton nodes\n",
    "G_temp.remove_nodes_from(list(nx.isolates(G_temp)))\n",
    "\n",
    "#Fetching universe of nodes\n",
    "universe = nx.get_node_attributes(G_temp, \"group\")\n",
    "\n",
    "centrality = nx.betweenness_centrality(G_temp)\n",
    "true_centrality = {\"Female\": [], \"Male\":[], \"NaN\":[]}\n",
    "\n",
    "#Adding true centrality means\n",
    "for u,v in centrality.items():\n",
    "    true_centrality[universe[u]].append(v)\n",
    "\n",
    "true_mean_female = np.mean(true_centrality['Female'])\n",
    "true_mean_male = np.mean(true_centrality['Male'])\n",
    "\n",
    "random_female = []\n",
    "random_male = []\n",
    "#Conducting permutation tests\n",
    "for _ in tqdm(range(N)):\n",
    "    labels = list(nx.get_node_attributes(G_temp, \"group\").values())\n",
    "    random_labels = np.random.permutation(labels)\n",
    "    universe = dict(zip(G_temp.nodes(),random_labels))\n",
    "    uni_centrality = {\"Female\": [], \"Male\":[], \"NaN\": []}\n",
    "    for u,v in centrality.items():\n",
    "        uni_centrality[universe[u]] += [v]\n",
    "\n",
    "    random_female.append(np.mean(uni_centrality['Female']))\n",
    "    random_male.append(np.mean(uni_centrality['Male']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e67f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "bins_female = np.linspace(min(random_female), max(random_female), 30) # TODO: adjust binning\n",
    "bins_male = np.linspace(min(random_male), max(random_male), 30)\n",
    "\n",
    "hist_female, edges_female = np.histogram(random_female, bins=bins_female)\n",
    "hist_male, edges_male = np.histogram(random_male, bins=bins_male)\n",
    "\n",
    "x_female = (edges_female[1:]+edges_female[:-1])/2\n",
    "x_male = (edges_male[1:]+edges_male[:-1])/2\n",
    "width_female = edges_female[1] - edges_female[0]\n",
    "width_male = edges_male[1] - edges_male[0]\n",
    "\n",
    "ax1.bar(x_female, hist_female, width=width_female*0.95)\n",
    "ax1.axvline(true_mean_female, c='r', label='Observed mean')\n",
    "ax2.bar(x_male, hist_male, width= width_male*0.95)\n",
    "ax2.axvline(true_mean_male, c='r', label='Observed mean')\n",
    "\n",
    "ax1.set_title(\"Female - Ensemble Histogram\")\n",
    "ax2.set_title(\"Male - Ensemble Histogram\")\n",
    "ax1.set_xlabel(\"Betweenness centrality\")\n",
    "ax2.set_xlabel(\"Betweenness centrality\")\n",
    "ax1.set_ylabel(\"counts\")\n",
    "ax2.set_ylabel(\"counts\")\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_female = len([i for i in random_female if i  > true_mean_female])/N\n",
    "pval_male = len([i for i in random_male if i > true_mean_male])/N\n",
    "pval_female, pval_male"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dabce46",
   "metadata": {},
   "source": [
    "Both of the tests yield a p-value belov the significance level $\\alpha=0.05$. Hence, indicating that the gender of the superhero does have a significant influence on the centrality of the superhero. We do see that the betweenness centrality is larger for men than for women also, indicating that male superheroes are often more central in the graph than female ones. Is this true in other years as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e30b3f",
   "metadata": {},
   "source": [
    "### Temporal permutation test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95fb32",
   "metadata": {},
   "source": [
    "Furthermore, to evaluate the influence of gender in the whole temporal network, the p-values were determined at each yearly snapshot of the network using the same approach as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(expanded_edgelist.timestamp.unique())\n",
    "true_means_female = []\n",
    "true_means_male = []\n",
    "pvals_female = []\n",
    "pvals_male = []\n",
    "pvals_nan = []\n",
    "N = 1000\n",
    "\n",
    "\n",
    "for year in tqdm(years):\n",
    "    edgelist_temp = expanded_edgelist[expanded_edgelist.timestamp==year]\n",
    "    #attrs_temp = {row.source: {'group': row['source universe']} for idx, row in edgelist_temp.iterrows()}\n",
    "    attrs_temp = {row.source: {'group': gender_map[row.source]} for idx, row in edgelist_temp.iterrows()}\n",
    "    G_temp = nx.from_pandas_edgelist(edgelist_temp, create_using = nx.DiGraph)\n",
    "\n",
    "    #Adding attributes\n",
    "    nx.set_node_attributes(G_temp, attrs_temp)\n",
    "\n",
    "    #Fixing issue with nodes missing universe data\n",
    "    missing_attrs = {row.target: {'group': gender_map[row.target]}for idx, row in edgelist_temp[edgelist_temp['target'].isin([u for u,v in G_temp.nodes(data=True) if not v])].iterrows()}\n",
    "    nx.set_node_attributes(G_temp, missing_attrs)\n",
    "\n",
    "\n",
    "    #Removing singleton nodes\n",
    "    G_temp.remove_nodes_from(list(nx.isolates(G_temp)))\n",
    "\n",
    "    #Fetching universe of nodes\n",
    "    universe = nx.get_node_attributes(G_temp, \"group\")\n",
    "\n",
    "    centrality = nx.betweenness_centrality(G_temp)\n",
    "    true_centrality = {\"Female\": [], \"Male\":[], \"NaN\":[]}\n",
    "\n",
    "    #Adding true centrality means\n",
    "    for u,v in centrality.items():\n",
    "        true_centrality[universe[u]].append(v)\n",
    "\n",
    "    true_mean_female = np.mean(true_centrality['Female'])\n",
    "    true_mean_male = np.mean(true_centrality['Male'])\n",
    "    true_mean_nan = np.mean(true_centrality['NaN'])\n",
    "\n",
    "    random_female = []\n",
    "    random_male = []\n",
    "    random_nan = []\n",
    "    #Conducting permutation tests\n",
    "    for _ in range(N):\n",
    "        labels = list(nx.get_node_attributes(G_temp, \"group\").values())\n",
    "        random_labels = np.random.permutation(labels)\n",
    "        universe = dict(zip(G_temp.nodes(),random_labels))\n",
    "        uni_centrality = {\"Female\": [], \"Male\":[], \"NaN\": []}\n",
    "        for u,v in centrality.items():\n",
    "            uni_centrality[universe[u]] += [v]\n",
    "\n",
    "        random_female.append(np.mean(uni_centrality['Female']))\n",
    "        random_male.append(np.mean(uni_centrality['Male']))\n",
    "        random_nan.append(np.mean(uni_centrality['NaN']))\n",
    "\n",
    "    #Computing pvals\n",
    "    pval_female = len([i for i in random_female if i  >= true_mean_female])/N\n",
    "    pval_male = len([i for i in random_male if i >= true_mean_male])/N\n",
    "    pval_nan = len([i for i in random_nan if i >= true_mean_nan])/N\n",
    "    \n",
    "    pvals_female.append(pval_female)\n",
    "    pvals_male.append(pval_male)\n",
    "    pvals_nan.append(pval_nan)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce7663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,4), dpi=100)\n",
    "ax.plot([str(year) for year in years], pvals_female, label='Female', c='darkblue')\n",
    "ax.plot([str(year) for year in years], pvals_male, label='Male', c='firebrick')\n",
    "ax.plot([str(year) for year in years], pvals_nan, label='NaN', c='orange')\n",
    "ax.axhline(0.05, c='r', linestyle='--', label=r\"$\\alpha=0.05$\")\n",
    "ax.set_xlim(['2001','2022'])\n",
    "ax.legend()\n",
    "ax.set_title(\"p-values of Gender Label Permutation Test (2001-2022)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda3f9f",
   "metadata": {},
   "source": [
    "We see here that the betweenness centrality of males is significantly different from that of a graph with random labels every year apart from 2003. Females' betweenness centrality is only significant after 2006.\n",
    "\n",
    "Well, what does this mean? It means that the average betweenness centrality of females was only significantly higher than the average betweenness centrality of the networks with permuted gender labels after 2006, which implies that the node property female had no significant influence on node centrality before 2006.\n",
    "\n",
    "We can, however, compare it to the unlabeled data. It is never significantly higher than a graph with permuted labels, which can be because the betweenness score is lower for unlabeled data; we tend to have metadata on popular nodes with high betweenness centrality and not on less popular ones. We can investigate this by doing a one-sided permutation test from the other side - checking the significance that the centrality of unlabeled data is lower than that of randomly permuted labels!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(expanded_edgelist.timestamp.unique())\n",
    "true_means_female = []\n",
    "true_means_male = []\n",
    "pvals_female = []\n",
    "pvals_male = []\n",
    "pvals_nan = []\n",
    "N = 1000\n",
    "\n",
    "\n",
    "for year in tqdm(years):\n",
    "    edgelist_temp = expanded_edgelist[expanded_edgelist.timestamp==year]\n",
    "    #attrs_temp = {row.source: {'group': row['source universe']} for idx, row in edgelist_temp.iterrows()}\n",
    "    attrs_temp = {row.source: {'group': gender_map[row.source]} for idx, row in edgelist_temp.iterrows()}\n",
    "    G_temp = nx.from_pandas_edgelist(edgelist_temp, create_using = nx.DiGraph)\n",
    "\n",
    "    #Adding attributes\n",
    "    nx.set_node_attributes(G_temp, attrs_temp)\n",
    "\n",
    "    #Fixing issue with nodes missing universe data\n",
    "    missing_attrs = {row.target: {'group': gender_map[row.target]}for idx, row in edgelist_temp[edgelist_temp['target'].isin([u for u,v in G_temp.nodes(data=True) if not v])].iterrows()}\n",
    "    nx.set_node_attributes(G_temp, missing_attrs)\n",
    "\n",
    "\n",
    "    #Removing singleton nodes\n",
    "    G_temp.remove_nodes_from(list(nx.isolates(G_temp)))\n",
    "\n",
    "    #Fetching universe of nodes\n",
    "    universe = nx.get_node_attributes(G_temp, \"group\")\n",
    "\n",
    "    centrality = nx.betweenness_centrality(G_temp)\n",
    "    true_centrality = {\"Female\": [], \"Male\":[], \"NaN\":[]}\n",
    "\n",
    "    #Adding true centrality means\n",
    "    for u,v in centrality.items():\n",
    "        true_centrality[universe[u]].append(v)\n",
    "\n",
    "    true_mean_female = np.mean(true_centrality['Female'])\n",
    "    true_mean_male = np.mean(true_centrality['Male'])\n",
    "    true_mean_nan = np.mean(true_centrality['NaN'])\n",
    "\n",
    "    random_female = []\n",
    "    random_male = []\n",
    "    random_nan = []\n",
    "    #Conducting permutation tests\n",
    "    for _ in range(N):\n",
    "        labels = list(nx.get_node_attributes(G_temp, \"group\").values())\n",
    "        random_labels = np.random.permutation(labels)\n",
    "        universe = dict(zip(G_temp.nodes(),random_labels))\n",
    "        uni_centrality = {\"Female\": [], \"Male\":[], \"NaN\": []}\n",
    "        for u,v in centrality.items():\n",
    "            uni_centrality[universe[u]] += [v]\n",
    "\n",
    "        random_female.append(np.mean(uni_centrality['Female']))\n",
    "        random_male.append(np.mean(uni_centrality['Male']))\n",
    "        random_nan.append(np.mean(uni_centrality['NaN']))\n",
    "\n",
    "    #Computing pvals\n",
    "    pval_female = len([i for i in random_female if i  <= true_mean_female])/N\n",
    "    pval_male = len([i for i in random_male if i <= true_mean_male])/N\n",
    "    pval_nan = len([i for i in random_nan if i <= true_mean_nan])/N\n",
    "    \n",
    "    pvals_female.append(pval_female)\n",
    "    pvals_male.append(pval_male)\n",
    "    pvals_nan.append(pval_nan)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c095296",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,4), dpi=100)\n",
    "#ax.plot([str(year) for year in years], pvals_female, label='Female', c='darkblue')\n",
    "#ax.plot([str(year) for year in years], pvals_male, label='Male', c='firebrick')\n",
    "ax.plot([str(year) for year in years], pvals_nan, label='NaN', c='orange')\n",
    "ax.axhline(0.05, c='r', linestyle='--', label=r\"$\\alpha=0.05$\")\n",
    "ax.set_xlim(['2001','2022'])\n",
    "ax.legend()\n",
    "ax.set_title(\"p-values of Gender Label Permutation Test (2001-2022)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492bf184",
   "metadata": {},
   "source": [
    "We see that unlabeled data do indeed tend to have a lower centrality than randomly permuted labels! This is a clear bias in our meta data.\n",
    "\n",
    "This trend should not be the same in race as we do not have data on quite a few popular nodes (eg. Thor, Batman, Superman). Let's check it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(expanded_edgelist.timestamp.unique())\n",
    "true_means_latino = []\n",
    "true_means_black = []\n",
    "pvals_latino = []\n",
    "pvals_black = []\n",
    "pvals_asian = []\n",
    "pvals_nan = []\n",
    "pvals_nan_under = []\n",
    "N = 1000\n",
    "\n",
    "race_map = {}\n",
    "for idx, row in meta_df.iterrows():  \n",
    "    if row['black'] == 1:\n",
    "        gender_map[row.WikiLink] = \"black\"\n",
    "    elif row['asian'] == 1:\n",
    "        gender_map[row.WikiLink] = \"asian\"\n",
    "    elif row['latino'] == 1:\n",
    "        gender_map[row.WikiLink] = \"latino\"\n",
    "    else:\n",
    "        gender_map[row.WikiLink] = \"NaN\"\n",
    "\n",
    "for year in tqdm(years):\n",
    "    edgelist_temp = expanded_edgelist[expanded_edgelist.timestamp==year]\n",
    "    #attrs_temp = {row.source: {'group': row['source universe']} for idx, row in edgelist_temp.iterrows()}\n",
    "    attrs_temp = {row.source: {'group': gender_map[row.source]} for idx, row in edgelist_temp.iterrows()}\n",
    "    G_temp = nx.from_pandas_edgelist(edgelist_temp, create_using = nx.DiGraph)\n",
    "\n",
    "    #Adding attributes\n",
    "    nx.set_node_attributes(G_temp, attrs_temp)\n",
    "\n",
    "    #Fixing issue with nodes missing universe data\n",
    "    missing_attrs = {row.target: {'group': gender_map[row.target]}for idx, row in edgelist_temp[edgelist_temp['target'].isin([u for u,v in G_temp.nodes(data=True) if not v])].iterrows()}\n",
    "    nx.set_node_attributes(G_temp, missing_attrs)\n",
    "\n",
    "\n",
    "    #Removing singleton nodes\n",
    "    G_temp.remove_nodes_from(list(nx.isolates(G_temp)))\n",
    "\n",
    "    #Fetching universe of nodes\n",
    "    universe = nx.get_node_attributes(G_temp, \"group\")\n",
    "\n",
    "    centrality = nx.betweenness_centrality(G_temp)\n",
    "    true_centrality = {\"latino\": [], \"black\":[],\"asian\":[], \"NaN\":[]}\n",
    "\n",
    "    #Adding true centrality means\n",
    "    for u,v in centrality.items():\n",
    "        true_centrality[universe[u]].append(v)\n",
    "\n",
    "    true_mean_latino = np.mean(true_centrality['latino'])\n",
    "    true_mean_black = np.mean(true_centrality['black'])\n",
    "    true_mean_asian = np.mean(true_centrality['asian'])\n",
    "    true_mean_nan = np.mean(true_centrality['NaN'])\n",
    "\n",
    "    random_latino = []\n",
    "    random_black = []\n",
    "    random_asian = []\n",
    "    random_nan = []\n",
    "    #Conducting permutation tests\n",
    "    for _ in range(N):\n",
    "        labels = list(nx.get_node_attributes(G_temp, \"group\").values())\n",
    "        random_labels = np.random.permutation(labels)\n",
    "        universe = dict(zip(G_temp.nodes(),random_labels))\n",
    "        uni_centrality = {\"latino\": [], \"black\":[],\"asian\":[], \"NaN\": []}\n",
    "        for u,v in centrality.items():\n",
    "            uni_centrality[universe[u]] += [v]\n",
    "\n",
    "        random_latino.append(np.mean(uni_centrality['latino']))\n",
    "        random_black.append(np.mean(uni_centrality['black']))\n",
    "        random_asian.append(np.mean(uni_centrality['asian']))\n",
    "        random_nan.append(np.mean(uni_centrality['NaN']))\n",
    "\n",
    "    #Computing pvals\n",
    "    pval_latino = len([i for i in random_latino if i  >= true_mean_latino])/N\n",
    "    pval_black = len([i for i in random_black if i >= true_mean_black])/N\n",
    "    pval_asian = len([i for i in random_asian if i >= true_mean_asian])/N\n",
    "    pval_nan = len([i for i in random_nan if i >= true_mean_nan])/N\n",
    "    pval_nan_under = len([i for i in random_nan if i <= true_mean_nan])/N\n",
    "    \n",
    "    pvals_latino.append(pval_latino)\n",
    "    pvals_black.append(pval_black)\n",
    "    pvals_asian.append(pval_asian)\n",
    "    pvals_nan_under.append(pval_nan_under)\n",
    "    pvals_nan.append(pval_nan)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,4), dpi=100)\n",
    "ax.plot([str(year) for year in years], pvals_latino, label='Latino', c='darkblue')\n",
    "ax.plot([str(year) for year in years], pvals_black, label='Black', c='firebrick')\n",
    "ax.plot([str(year) for year in years], pvals_asian, label='Asian', c='purple')\n",
    "ax.plot([str(year) for year in years], pvals_nan, label='NaN', c='orange')\n",
    "ax.axhline(0.05, c='r', linestyle='--', label=r\"$\\alpha=0.05$\")\n",
    "ax.set_xlim(['2001','2022'])\n",
    "ax.legend()\n",
    "ax.set_title(\"p-values of Race Label Permutation Test (2001-2022)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef01a8e",
   "metadata": {},
   "source": [
    "This is interesting. Black and Asian labels seem to have a significantly higher node centrality than random labels. Latino generally does not. This suggests that most Latino characters are not as central to the universes as blacks and asians.\n",
    "\n",
    "It also seems that NaN is actually significant here as well. Apparently the same bias seems to be in place here as with gender; we only have meta data for popular superheroes.\n",
    "\n",
    "Let's check the plot for NaN centrality to be under random centrality;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e08d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,4), dpi=100)\n",
    "ax.plot([str(year) for year in years], pvals_nan_under, label='NaN', c='orange')\n",
    "ax.axhline(0.05, c='r', linestyle='--', label=r\"$\\alpha=0.05$\")\n",
    "ax.set_xlim(['2001','2022'])\n",
    "ax.legend()\n",
    "ax.set_title(\"p-values of Race Label Permutation Test (2001-2022)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be57e62f",
   "metadata": {},
   "source": [
    "NaN centrality is significantly lower than random. When creating our meta data we found something weird; most superheroes have a black or asian version. This was true for Batman and Superman at least. The links to these superheroes were not equal to the links that we had in our data however. This was because they linked to specific subsections in the Wiki pages where the superhero might have been of another race.\n",
    "\n",
    "Therefore, we measured document similarity using shingles of the links and jaccard similarity between them. Only shingles that were 90% or more similar were given the attribute. So sometimes a very popular superhero that is mostly white (eg. Batman or Superman) might be given the attribute of a racial minority.\n",
    "\n",
    "Because we would expect most superheroes that are white to have a high betweenness centrality, we would expect the yellow line above to be insignificant. However, if many high-betweenness-superheroes slipped through the jaccard similarity measure to be a minority this actually flaws our data.\n",
    "\n",
    "So one of three things could have happened:\n",
    "\n",
    ">1. Many very central white superheroes have gotten misclassified\n",
    ">2. The bias that most popular superheroes are given a label is very strong in this situation.\n",
    ">3. We are wrong about white superheroes generally having a higher betweenness centrality.\n",
    "\n",
    "\n",
    "Although the third option might entail some interesting discussions we can almost certainly rule that out because other studies have shown that there are many more white superheroes that are essential to movies and comics.\n",
    "\n",
    "The second option could still be true, however, we would still have more than a few highly central nodes that were left out and would give the non-labeled nodes a much higher average centrality.\n",
    "\n",
    "Therefore, it could be a combination of the first and second option that explains this behaviour. This means that we can probably not really say anything about the two plots above. Maybe we will investigate this in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f16c82",
   "metadata": {},
   "source": [
    "## Centrality Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbc3be",
   "metadata": {},
   "source": [
    "The previous section attempted to investigate how the different attributes of the superheroes influence the centrality of the corresponding nodes. To further investigate the temporal evolution of the network, we will investigate the stability of nodes' eigenvector centrality as a function of time using the Pearson correlation. More specifically, the Pearson correlation between the eigenvector centrality of nodes at a given time step and the remaining time steps will be determined [2].\n",
    "\n",
    "A subset of nodes that are continuously present throughout the time interval has to be found to compute the Pearson correlation correctly. Given that the majority of the nodes are present in the year 2007, the investigation will be limited to the period 2007 to 2022. The computed Pearson correlations are illustrated using a heatmap with years on the x- and y-axis such that the off-diagonal elements correspond to the estimated correlation between the node centralities from two different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_2007 = edgelist[edgelist.timestamp==2007]\n",
    "G_2007 = nx.from_pandas_edgelist(edgelist_2007, create_using=nx.DiGraph)\n",
    "nodes = list(G_2007.nodes())\n",
    "\n",
    "edgelist_2022 = edgelist[edgelist.timestamp==2022]\n",
    "G_2022 = nx.from_pandas_edgelist(edgelist_2022, create_using=nx.DiGraph)\n",
    "nodes_2022 = list(G_2022.nodes())\n",
    "\n",
    "temp_list = []\n",
    "for year in years[6:]:\n",
    "    edgelist_temp = expanded_edgelist[expanded_edgelist.timestamp==year]\n",
    "    G_temp = nx.from_pandas_edgelist(edgelist_temp, create_using=nx.DiGraph)\n",
    "    temp_list.append(list(G_temp.nodes))\n",
    "\n",
    "nodes_final = list(set(temp_list[0]) & set(temp_list[1]) \n",
    "    & set(temp_list[2]) & set(temp_list[3])\n",
    "    & set(temp_list[4]) & set(temp_list[5])\n",
    "    & set(temp_list[6]) & set(temp_list[7])\n",
    "    & set(temp_list[8]) & set(temp_list[9])\n",
    "    & set(temp_list[10]) & set(temp_list[11])\n",
    "    & set(temp_list[12]) & set(temp_list[13])\n",
    "    & set(temp_list[14]) & set(temp_list[15]))\n",
    "\n",
    "eigenvals_centrality = np.zeros((len(nodes_final), len(temp_list)))\n",
    "for idx, year in enumerate(years[6:]):\n",
    "    edgelist_temp = expanded_edgelist[expanded_edgelist.timestamp==year]\n",
    "    G_temp = nx.from_pandas_edgelist(edgelist_temp, create_using=nx.DiGraph)\n",
    "    centrality_dict = nx.eigenvector_centrality(G_temp)\n",
    "    #Getting nodes of interest\n",
    "    eigenvals_centrality[:,idx] = [centrality_dict[node] for node in nodes_final if node in list(G_temp.nodes())]\n",
    "\n",
    "corr = np.corrcoef(eigenvals_centrality, rowvar=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10), dpi=100)\n",
    "sns.heatmap(corr, xticklabels=years[6:], yticklabels=years[6:], ax=ax)\n",
    "ax.set_title(\"Pearson correlation between eigenvector centralities\")\n",
    "ax.tick_params(axis='y', rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d836277",
   "metadata": {},
   "source": [
    "The heatmap correlation demonstrates that the Pearson correlation between node centralities of two different years depends on the time difference between the two years. For instance, by looking at the node centralities determined from 2007 and comparing the computed Pearson correlations with the remaining years, it becomes evident that the correlation decreases with every year passing. The figure demonstrates a generally high correlation between the node centralities from consecutive years. Interestingly, the figure also illustrates emerging block structures near the diagonal increasing in size with time. This trend indicates that the stability of eigenvector centrality increases with time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbcf41",
   "metadata": {},
   "source": [
    "## Clusters! How Do They Evolve?\n",
    "In this section, the first part will try to investigate the hypothesis:\n",
    "> * The network grows in a \"coral-growth\" pattern. \n",
    ">  - This will be tested by running an InfoMap Ensemble algorithm to determine communities in the graph at $t_n$. Then we will create a heatmap showing when nodes are added to each community to see whether they are added in a coral pattern - meaning that nodes within the same community are added at the same time. Spoiler alert: We will try to find an even cooler way to show this during our project!\n",
    "\n",
    "Subsequently, we will try to visualise how the greatest communities have evolved through time by illustrating the node migration between the greatest communities at each timestep. Hopefully, this will provide insights into answering the following questions:\n",
    "> * Does the communities of the temporal network demonstrate common characteristics w.r.t community evolution [3]?\n",
    ">  - Growth/Contraction\n",
    ">  - Merging/Splitting\n",
    ">  - Birth/death\n",
    "> * How stable are the communities in the temporal Wikipedia network as a function of time?\n",
    "> * What specific periods demonstrate significant rewiring of the network diagram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e802089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_louvain(graph, N, k, threshold = 0.9):\n",
    "    ''' The same algorithm is used with Infomap later!\n",
    "    1) run several trials of Louvain on the same network,\n",
    "    2) built a new network where a pair of the original nodes is linked if their total co-membership across all the Louvain trials is above a given threshold (e.g., 90%),\n",
    "    3) identify the disjoints sets which represent the resulting communities.'''\n",
    "    from community import community_louvain \n",
    "    import random\n",
    "    from collections import Counter\n",
    "    from collections import defaultdict\n",
    "    import string\n",
    "    from itertools import combinations\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    #step 1\n",
    "    tot_communities = []\n",
    "    for iter_ in tqdm(range(N), disable=True):\n",
    "        \n",
    "        partition = community_louvain.best_partition(graph)\n",
    "        tot_communities.append(partition)\n",
    "    \n",
    "    #step 2\n",
    "    G_new = nx.Graph() #Don't think this has to be directed\n",
    "    node_pairs = combinations(list(graph.nodes), 2)\n",
    "    for (u,v) in tqdm(node_pairs, disable=True):\n",
    "        co_occurence = 0\n",
    "        for partitions in tot_communities:\n",
    "            if partitions[u] == partitions[v]: \n",
    "                co_occurence += 1\n",
    "        if co_occurence/N > threshold:\n",
    "            G_new.add_edge(u, v)\n",
    "        \n",
    "        \n",
    "    #step 3\n",
    "    k_greatest_components = {i: list(community) for i, community in enumerate(sorted(nx.connected_components(G_new), key=len, reverse=True)[:k])}\n",
    "    \n",
    "    \n",
    "    return k_greatest_components\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = graphs[2022].copy().to_undirected()\n",
    "\n",
    "# split into marvel and DC\n",
    "marvel = []\n",
    "dc = []\n",
    "for key, value in nx.get_node_attributes(G,'Universe').items():\n",
    "    if value == 'Marvel':\n",
    "        marvel.append(key)\n",
    "    elif value == 'DC':\n",
    "        dc.append(key)\n",
    "\n",
    "G_marvel = G.subgraph(marvel)\n",
    "G_dc = G.subgraph(dc)\n",
    "\n",
    "#Keep only largest component\n",
    "G_marvel = G_marvel.subgraph(sorted(nx.connected_components(G_marvel), key=len, reverse=True)[0])\n",
    "G_dc = G_dc.subgraph(sorted(nx.connected_components(G_dc), key=len, reverse=True)[0])\n",
    "\n",
    "# compute the best partition\n",
    "k = 10 #N communities in each subgraph!\n",
    "partition_marvel = ensemble_louvain(G_marvel,100,k)\n",
    "partition_marvel = {value[i]: key for key, value in partition_marvel.items() for i in range(len(value))}\n",
    "partition_dc = ensemble_louvain(G_dc,100,k)\n",
    "partition_dc = {value[i]: key for key, value in partition_dc.items() for i in range(len(value))}\n",
    "n_marvel_communities = np.unique(list(partition_marvel.values()))\n",
    "n_dc_communities = np.unique(list(partition_dc.values()))\n",
    "\n",
    "\n",
    "dc_pss = {}\n",
    "marvel_pss = {}\n",
    "for key in graphs.keys():\n",
    "    dc_ps = []\n",
    "    marvel_ps = []\n",
    "    for node in graphs[key].nodes():\n",
    "        try:\n",
    "            graphs[key-1].nodes()[node]\n",
    "            continue\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            p_number_marvel = partition_marvel[node]\n",
    "            marvel_ps.append(p_number_marvel)\n",
    "        except:\n",
    "            try:\n",
    "                p_number_dc = partition_dc[node]\n",
    "                dc_ps.append(p_number_dc)\n",
    "            except:\n",
    "                continue\n",
    "    dc_pss[key] = dc_ps\n",
    "    marvel_pss[key] = marvel_ps\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "dc_counts = {}\n",
    "for key, value in dc_pss.items():\n",
    "    dc_counts[key] = Counter(value)\n",
    "marvel_counts = {}\n",
    "for key, value in marvel_pss.items():\n",
    "    marvel_counts[key] = Counter(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0deea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of communities shown\n",
    "n_communities=k*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_matrix = np.zeros((n_communities,22))\n",
    "count = 0\n",
    "for key, value in dc_counts.items():\n",
    "    for key2, value2 in value.items():\n",
    "        partition_matrix[key2+k,count] = value2\n",
    "    count += 1\n",
    "count = 0\n",
    "for key, value in marvel_counts.items():\n",
    "    for key2, value2 in value.items():\n",
    "        partition_matrix[key2,count] = value2\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c40f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#normalize matrix\n",
    "partition_matrix_normalised = normalize(partition_matrix, axis=0, norm='l1')\n",
    "partition_matrix_normalisedrow = normalize(partition_matrix, axis=1, norm='l1')\n",
    "\n",
    "fig, ax = plt.subplots(2,figsize=(8,10), dpi=100)\n",
    "df_partition_matrix_normalised = pd.DataFrame(partition_matrix_normalised, index = np.arange(0,n_communities,1),\n",
    "                  columns = [i for i in graphs.keys()])\n",
    "sns.heatmap(df_partition_matrix_normalised, ax=ax[0])\n",
    "ax[0].set_xticklabels([i for i in graphs.keys()],rotation=45)\n",
    "\n",
    "df_partition_matrix_normalised = pd.DataFrame(partition_matrix_normalisedrow, index = np.arange(0,n_communities,1),\n",
    "                  columns = [i for i in graphs.keys()])\n",
    "sns.heatmap(df_partition_matrix_normalised, ax=ax[1])\n",
    "ax[1].set_xticklabels([i for i in graphs.keys()],rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0aed6",
   "metadata": {},
   "source": [
    "In order to better capture communities in the distinct universes - and not just two overall communities being Marvel and DC - we separate the two by their 'universe' attribute. Here we take the 10 largest communities formed. The first 10 are Marvel communities and the last 10 are DC communities. Because we do an ensemble Louvain partitioning, we will see that a lot of nodes are not in these communities as they would shift too much between communities based on the Louvain seeding. We do this because Louvain is non-determenistic and we want to be sure that we don't see a lucky - or unlucky - seed determine the communities.\n",
    "\n",
    "The first plot is normalised by years (columns) therefore, whenever we have a high value, we see that most of the nodes added that year was added to that community. Especially in the later years we do see a form of coral-growth in the Marvel communities. DC communities are overshadowed quite much by Marvel. This suggests that Marvel communities are far more popular and have many more nodes added to the communities during later years.\n",
    "\n",
    "The second plot is normalised row-wise. Here we see the same trend as with everything else in this notebook... Let's try to look at 2009 and onwards and see if we find something more interesting. We will also add 0.5 to all values above 0 in the first plot. This way we can more easily see when there is a node added to a community!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec404713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize matrix\n",
    "added = np.zeros(partition_matrix.shape)\n",
    "for i in range(len(partition_matrix)):\n",
    "    for j in range(partition_matrix.shape[1]):\n",
    "        if partition_matrix[i,j]>0:\n",
    "            added[i,j] = 0.5\n",
    "partition_matrix_normalised = added + normalize(partition_matrix, axis=0, norm='l1')\n",
    "partition_matrix_normalisedrow = normalize(partition_matrix[:,8:], axis=1, norm='l1')\n",
    "\n",
    "fig, ax = plt.subplots(2,figsize=(8,10), dpi=100)\n",
    "df_partition_matrix_normalised = pd.DataFrame(partition_matrix_normalised, index = np.arange(0,n_communities,1),\n",
    "                  columns = [i for i in graphs.keys()])\n",
    "sns.heatmap(df_partition_matrix_normalised, ax=ax[0])\n",
    "ax[0].set_xticklabels([i for i in graphs.keys()],rotation=45)\n",
    "\n",
    "df_partition_matrix_normalised = pd.DataFrame(partition_matrix_normalisedrow, index = np.arange(0,n_communities,1),\n",
    "                  columns = [i for i in graphs.keys() if int(i)>=2009])\n",
    "sns.heatmap(df_partition_matrix_normalised, ax=ax[1])\n",
    "ax[1].set_xticklabels([i for i in graphs.keys() if int(i)>=2009],rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ebf15",
   "metadata": {},
   "source": [
    "The first plot indicates that there are just not that many nodes added to DC communities after 'the big boom' in Wikipedia. Some communities are more steadily added to - like community 1 - while others are added to in small bursts - like community 4 or 9. \n",
    "\n",
    "TODO: Christian læs lige det her!\n",
    "\n",
    "We also see this in the second plot where community 1 and 2 have a darker colour because they are more steadily added to while other communities have brighter colours meaning nodes are added to them in bursts.\n",
    "\n",
    "In the project we will investigate a way to test this with permutation. We will also look at revisions to see if we see the same thing.\n",
    "\n",
    "### Look at These Gifs!\n",
    "Now we will check the size and strength of the communities we found!\n",
    "\n",
    "First gif will show the size! Sorry for the code.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85791fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get community names (largest degree node in each community)\n",
    "d = dict(graphs[2022].degree())\n",
    "sorted_degrees = {k: d[k] for k in sorted(d, key=d.get, reverse=True)}\n",
    "\n",
    "community_names = {}\n",
    "for key,value in sorted_degrees.items():\n",
    "    try:\n",
    "        com = partition_marvel[key]\n",
    "        try:\n",
    "            community_names[com]\n",
    "        except:\n",
    "            community_names[com] = key\n",
    "    except:\n",
    "        try:\n",
    "            com = partition_dc[key]\n",
    "            try:\n",
    "                connumity_names[com]\n",
    "            except:\n",
    "                community_names[com+len(n_marvel_communities)] = key\n",
    "        except:\n",
    "            continue\n",
    "    if len(community_names)==n_communities:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_colors(N:int,seed:int):\n",
    "    '''\n",
    "    N: number of colors you want to generate\n",
    "    seed: seed\n",
    "    colors: list of random hex colors\n",
    "    '''\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    colors = []\n",
    "    for i in range(N):\n",
    "        colors.append(\"#%06x\" % random.randint(0, 0xFFFFFF))\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize matrix\n",
    "added = np.zeros(partition_matrix.shape)\n",
    "for i in range(len(partition_matrix)):\n",
    "    for j in range(partition_matrix.shape[1]):\n",
    "        if partition_matrix[i,j]>0:\n",
    "            added[i,j] = 0.5\n",
    "partition_matrix_normalised = added + normalize(partition_matrix, axis=0, norm='l1')\n",
    "partition_matrix_normalisedrow = normalize(partition_matrix[:,8:], axis=1, norm='l1')\n",
    "\n",
    "fig, ax = plt.subplots(2,figsize=(8,10), dpi=100)\n",
    "df_partition_matrix_normalised = pd.DataFrame(partition_matrix_normalised, index = np.arange(0,n_communities,1),\n",
    "                  columns = [i for i in graphs.keys()])\n",
    "sns.heatmap(df_partition_matrix_normalised, ax=ax[0])\n",
    "ax[0].set_xticklabels([i for i in graphs.keys()],rotation=45)\n",
    "\n",
    "df_partition_matrix_normalised = pd.DataFrame(partition_matrix_normalisedrow, index = np.arange(0,n_communities,1),\n",
    "                  columns = [i for i in graphs.keys() if int(i)>=2009])\n",
    "sns.heatmap(df_partition_matrix_normalised, ax=ax[1])\n",
    "ax[1].set_xticklabels([i for i in graphs.keys() if int(i)>=2009],rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f0ea2d",
   "metadata": {},
   "source": [
    "The first plot indicates that there are just not that many nodes added to DC communities after 'the big boom' in Wikipedia. Some communities are more steadily added to - like community 1 - while others are added to in small bursts - like community 4 or 9. \n",
    "\n",
    "TODO: Christian læs lige det her!\n",
    "\n",
    "We also see this in the second plot where community 1 and 2 have a darker colour because they are more steadily added to while other communities have brighter colours meaning nodes are added to them in bursts.\n",
    "\n",
    "In the project we will investigate a way to test this with permutation. We will also look at revisions to see if we see the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdcce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get community names (largest degree node in each community)\n",
    "d = dict(graphs[2022].degree())\n",
    "sorted_degrees = {k: d[k] for k in sorted(d, key=d.get, reverse=True)}\n",
    "\n",
    "community_names = {}\n",
    "for key,value in sorted_degrees.items():\n",
    "    try:\n",
    "        com = partition_marvel[key]\n",
    "        try:\n",
    "            community_names[com]\n",
    "        except:\n",
    "            community_names[com] = key\n",
    "    except:\n",
    "        try:\n",
    "            com = partition_dc[key]\n",
    "            try:\n",
    "                connumity_names[com]\n",
    "            except:\n",
    "                community_names[com+len(n_marvel_communities)] = key\n",
    "        except:\n",
    "            continue\n",
    "    if len(community_names)==n_communities:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_colors(N:int,seed:int):\n",
    "    '''\n",
    "    N: number of colors you want to generate\n",
    "    seed: seed\n",
    "    colors: list of random hex colors\n",
    "    '''\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    colors = []\n",
    "    for i in range(N):\n",
    "        colors.append(\"#%06x\" % random.randint(0, 0xFFFFFF))\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c00402",
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcenodes = pd.DataFrame(np.array(list(edgelist.groupby(['source','timestamp']).groups.keys())), columns = ['node','timestamp'])\n",
    "targetnodes = pd.DataFrame(np.array(list(edgelist.groupby(['target','timestamp']).groups.keys())), columns = ['node','timestamp'])\n",
    "#keep unique nodes and timestamps!\n",
    "nodes =  pd.concat([sourcenodes,targetnodes]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "temp = meta_df.rename(columns={'WikiLink':'node'})\n",
    "nodes = pd.merge(nodes, temp, on=\"node\")\n",
    "nodes = nodes.drop(columns=['Unnamed: 0'])\n",
    "nodes.head()\n",
    "\n",
    "count = 0\n",
    "for year in graphs.keys():\n",
    "    fig, ax = plt.subplots(figsize=(8,5),dpi=100)\n",
    "    ax.bar([list(nodes[nodes['node']==list(community_names.values())[k]]['CharacterName'])[0] for k in range(len(list(community_names.values())))], \n",
    "           partition_matrix[list(community_names.keys()),:count].sum(1),\n",
    "          color=gen_colors(n_communities,42))\n",
    "    ax.set_ylabel(\"Counts\")\n",
    "    ax.set_xlabel(\"Community Name\")\n",
    "    ax.set_title(f\"Size of Each Community in {year}\")\n",
    "    plt.ylim((0,320))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f'plots/bar_plot_{year}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os\n",
    "import re\n",
    "\n",
    "with imageio.get_writer('bar_plot_finals.gif', mode='I', fps=0.5) as writer:\n",
    "    for year in graphs.keys():\n",
    "        image = imageio.imread(f'plots/bar_plot_{year}.png')\n",
    "        writer.append_data(image)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9deacc6",
   "metadata": {},
   "source": [
    "The gif looks like this:\n",
    "\n",
    "<img src=\"bar_plot_finals.gif\" width=\"750\" align=\"center\">\n",
    "\n",
    "If we look at the last 10 communities - the DC communities - we might notice that neither Batman nor Superman is among the nodes with the highest degrees in the communities. The only reason for this is because they are so central that they are put into different communities at each run of our ensemble Louvain method and therefore are held out of the final communities; they do not meet the threshold of being in the same community 90% of the time.\n",
    "\n",
    "To fix this we might lower the threshold, however, we would get more uncertain communities. Right now we are 90% certain that the communities we have found are true louvain communities.\n",
    "\n",
    "Other than that the gif shows us the development of the communities over time - taking the size of the communities into account.\n",
    "\n",
    "Next, we will look at the strength of the communities;\n",
    "\n",
    "$$Strength(c)=\\frac{intra}{(intra+inter)}$$\n",
    "\n",
    "Where $intra$ is the number of links that are intra connected (connecting two nodes of the same community, $c$) and $inter$ are links that are inter connected (connecting two nodes with different communities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc21bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "strength_years = {}\n",
    "for year in graphs.keys():\n",
    "    intra_links = {}\n",
    "    inter_links = {}\n",
    "    strength = {}\n",
    "    for key, value in partition_marvel.items():\n",
    "        try:\n",
    "            neighbors = list(graphs[year].neighbors(key)) + list(graphs[year].predecessors(key))\n",
    "            for neighbor in neighbors:\n",
    "                try:\n",
    "                    if partition_marvel[neighbor] == value or partition_dc[neighbor] == value:\n",
    "                        try:\n",
    "                            intra_links[value] += 1\n",
    "                            pass\n",
    "                        except:\n",
    "                            intra_links[value] = 1\n",
    "                            pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            inter_links[value] += 1\n",
    "                            pass\n",
    "                        except:\n",
    "                            inter_links[value] = 1\n",
    "                            pass\n",
    "                except:\n",
    "                    try:\n",
    "                        inter_links[value] += 1\n",
    "                        pass\n",
    "                    except:\n",
    "                        inter_links[value] = 1\n",
    "                        pass\n",
    "        except:\n",
    "            pass\n",
    "    for key, value in partition_marvel.items():\n",
    "        try:\n",
    "            strength[value] = intra_links[value]/ (intra_links[value]+inter_links[value])\n",
    "        except:\n",
    "            strength[value] = 0\n",
    "    #DC\n",
    "    for key, value in partition_dc.items():\n",
    "        value = value + k\n",
    "        try:\n",
    "            neighbors = list(graphs[year].neighbors(key)) + list(graphs[year].predecessors(key))\n",
    "            for neighbor in neighbors:\n",
    "                try:\n",
    "                    if partition_marvel[neighbor] == value-k or partition_dc[neighbor] == value-k:\n",
    "                        try:\n",
    "                            intra_links[value] += 1\n",
    "                            pass\n",
    "                        except:\n",
    "                            intra_links[value] = 1\n",
    "                            pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            inter_links[value] += 1\n",
    "                            pass\n",
    "                        except:\n",
    "                            inter_links[value] = 1\n",
    "                            pass\n",
    "                except:\n",
    "                    try:\n",
    "                        inter_links[value] += 1\n",
    "                        pass\n",
    "                    except:\n",
    "                        inter_links[value] = 1\n",
    "                        pass\n",
    "        except:\n",
    "            pass\n",
    "    for key, value in partition_dc.items():\n",
    "        value = value+k\n",
    "        try:\n",
    "            strength[value] = intra_links[value]/ (intra_links[value]+inter_links[value])\n",
    "        except:\n",
    "            strength[value] = 0\n",
    "    strength_years[year] = strength\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for year in graphs.keys():\n",
    "    fig, ax = plt.subplots(figsize=(8,5),dpi=100)\n",
    "    ax.bar([list(nodes[nodes['node']==list(community_names.values())[k]]['CharacterName'])[0] for k in range(len(list(community_names.values())))], \n",
    "           list(strength_years[year].values()),\n",
    "          color=gen_colors(n_communities,42))\n",
    "    ax.set_ylabel(\"Strength\")\n",
    "    ax.set_xlabel(\"Community Name\")\n",
    "    ax.set_title(f\"Strength in Each Community in {year}\")\n",
    "    plt.ylim((0, 1)) \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f'plots/bar_plot_strength_{year}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315bb63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os\n",
    "import re\n",
    "\n",
    "with imageio.get_writer('bar_plot_strength_finals.gif', mode='I', fps=0.5) as writer:\n",
    "    for year in graphs.keys():\n",
    "        image = imageio.imread(f'plots/bar_plot_strength_{year}.png')\n",
    "        writer.append_data(image)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116003ff",
   "metadata": {},
   "source": [
    "The gif looks like this:\n",
    "\n",
    "<img src=\"bar_plot_strength_finals.gif\" width=\"750\" align=\"center\">\n",
    "\n",
    "The Marvel communities seem to have a much higher strength than the DC communities. In general, neither of the communities reach a convincing strength due to the fact that we have an ensemble Louvain method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec6b90",
   "metadata": {},
   "source": [
    "## Node Migration\n",
    "The following section will investigate how the communities evolve in a temporal network by visualising the node migration between the communities determined at each snapshot. Therefore, an independent community detection will be conducted at each yearly static snapshot of the temporal network. \n",
    "\n",
    "The purpose of this investigation is to analyse how the temporal changes in the network impact the communities with an emphasis on growth/contraction, merging/splitting and birth/death of the superhero communities in the Wikipedia network through time. Visualising the node migration in an easily interpretable manner may help facilitate a greater understanding of which time-periods drastic changes in the community structures emerge throughout the lifespan of the temporal network. This understanding would then be beneficial to generate new questions and narrow the scope to significant periods.\n",
    "\n",
    "Determining the node migration based on independent community detections relies on a robust community detection algorithm that would satisfyingly identify the same communities if the wiring of the network remains unchanged. Hence, we decided to investigate different approaches to increase the stability of well-established community detection methods. One solution is to utilise an ensemble approach, in which several runs of a specific community detection algorithm are used to determine the communities. More specifically, we adopt the ensemble Infomap which is a modification of the ensemble Louvain algorithm presented in [1]:\n",
    "\n",
    "    1) Run N trials of Infomap using the same network\n",
    "\n",
    "    2) Create a new undirected network\n",
    "\n",
    "    3) Add edge between nodes u and v if the number of cooccurence in a specific community exceed threshold (e.g 90%) \n",
    "\n",
    "    4) Determine the connected components in the new network.\n",
    "\n",
    "\n",
    "The connected components of the new network correspond to the communities of the original network because, for each iteration, the Infomap assigns every node to a single community. Hence, if a threshold of 90% is used, then if an edge is added in the new network, it will imply that the two nodes u and v cooccur in the same community in more than 90% of the realisations. For the remaining realisations in which u and v can be assigned different communities, it can not be frequent enough to exceed the threshold (90% + 90% > 100%). Hence, the resulting connected components will correspond to the communities of the network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06596b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import plotly.express as px\n",
    "from infomap import Infomap\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34065ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Getting data\n",
    "df_marvel = pd.read_csv(\"files/marvel.csv\", index_col=0, encoding='utf8')\n",
    "df_marvel = df_marvel[df_marvel[\"WikiLink\"].notna()]\n",
    "df_dc = pd.read_csv(\"files/dc.csv\", index_col=0, encoding='utf8')\n",
    "df_dc = df_dc[df_dc[\"WikiLink\"].notna()]\n",
    "df_marvel['universe'] = 'Marvel'\n",
    "df_dc['universe'] = 'DC'\n",
    "df = pd.concat([df_marvel, df_dc], ignore_index=True, axis=0)\n",
    "\n",
    "edgelist = pd.read_csv(\"corrected_edgelist.csv\")\n",
    "temp = pd.merge(edgelist, df, left_on='source', right_on='WikiLink')\n",
    "temp = temp.rename(columns={\"universe\":\"source universe\"})\n",
    "temp = temp.drop(columns=['CharacterName', 'WikiLink'])\n",
    "temp = pd.merge(temp, df, left_on='target', right_on='WikiLink')\n",
    "temp = temp.rename(columns={\"universe\":\"target universe\"})\n",
    "temp = temp.drop(columns=['CharacterName', 'WikiLink'])\n",
    "expanded_edgelist = temp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55191e68",
   "metadata": {},
   "source": [
    "### Stability of Ensemble Infomap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e34630e",
   "metadata": {},
   "source": [
    "To investigate the usefulness of adopting an ensemble community detection approach, we examined the robustness of the partitions obtained with varying amounts of trials in the ensemble Infomap algorithm. More specifically, the ensemble Infomap was computed ten times with the number of trials equal to 1, 25, 50, 75, and 100. Subsequently, the adjusted rand index (ARI) was pairwisely determined for the obtained ten partitions at a given number of trials to evaluate the stability performance with varying number trials. To conduct this investigation the network at $t_{N}$ was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28bd354",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_temp = expanded_edgelist[expanded_edgelist.timestamp==2022]\n",
    "G_2022 = nx.from_pandas_edgelist(edgelist_temp, create_using = nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensemble_infomap(graph, N, k=None, threshold = 0.9, verbose=False, seed=0):\n",
    "    '''\n",
    "    1) run several trials of Infomap on the same network,\n",
    "    2) built a new network where a pair of the original nodes is linked if their total co-membership across all the Louvain trials is above a given threshold (e.g., 90%),\n",
    "    3) identify the disjoints sets which represent the resulting communities.'''\n",
    "    \n",
    "    #preperation\n",
    "    graph = nx.convert_node_labels_to_integers(graph, label_attribute=\"org_name\")\n",
    "    names = nx.get_node_attributes(graph, \"org_name\")\n",
    "    \n",
    "    #step 1\n",
    "    tot_communities = []\n",
    "    for iter_ in tqdm(range(N), disable=not verbose):\n",
    "        \n",
    "        im = Infomap(directed=True, silent=not verbose, seed=seed+iter_) #Using new seed\n",
    "        for edge in graph.edges():\n",
    "            im.addLink(*edge)\n",
    "        im.run()\n",
    "        communities = {}\n",
    "        for node in im.tree:\n",
    "            if node.is_leaf:\n",
    "                communities[names[node.node_id]] = node.module_id\n",
    "        tot_communities.append(communities)\n",
    "    \n",
    "    #preperation\n",
    "    graph = nx.relabel_nodes(graph, names)\n",
    "    \n",
    "    #step 2\n",
    "    G_new = nx.Graph()\n",
    "    G_new.add_nodes_from(list(graph.nodes))\n",
    "    node_pairs = combinations(list(graph.nodes), 2)\n",
    "    for (u,v) in tqdm(node_pairs, disable=not verbose):\n",
    "        co_occurence = 0\n",
    "        for partitions in tot_communities:\n",
    "            if partitions[u] == partitions[v]: \n",
    "                co_occurence += 1\n",
    "        if co_occurence/N > threshold:\n",
    "            G_new.add_edge(u, v)\n",
    "        \n",
    "        \n",
    "    #step 3\n",
    "    if k == None:\n",
    "        k = nx.number_connected_components(G_new)\n",
    "    k_greatest_components = {i: list(community) for i, community in enumerate(sorted(nx.connected_components(G_new), key=len, reverse=True)[:k])}\n",
    "    \n",
    "    \n",
    "    return k_greatest_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "import scipy.stats as st\n",
    "from itertools import combinations\n",
    "\n",
    "N = 10\n",
    "avg_ARI = []\n",
    "CI_ARI = []\n",
    "\n",
    "for n in tqdm(np.linspace(1,100,5)):\n",
    "    partitions = []\n",
    "    \n",
    "    for idx in range(N):\n",
    "        communities = ensemble_infomap(graph=G_2022, N=int(n), threshold=0.9, verbose=False, seed=(idx+N))\n",
    "        #Reversing dictionary\n",
    "        reversed_communities = {}\n",
    "        for u, v in communities.items():\n",
    "            for node in v:\n",
    "                reversed_communities[node] = u \n",
    "        \n",
    "        partitions.append(list(reversed_communities.values()))\n",
    "        #partitions = list(communities.values())\n",
    "        \n",
    "    partition_pairs = list(combinations(partitions, 2))\n",
    "    \n",
    "    ARIs = []\n",
    "    for (u, v) in partition_pairs:\n",
    "        #print(adjusted_rand_score(u,v))\n",
    "        ARIs.append(adjusted_rand_score(u,v))\n",
    "        \n",
    "    avg_ARI.append(np.mean(ARIs))\n",
    "    #std_ARI.append(np.std(ARIs))\n",
    "    CI_ARI.append(st.t.interval(confidence=0.95, df=N-1,\n",
    "                        loc=np.mean(ARIs),\n",
    "                        scale=st.sem(ARIs)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba224a",
   "metadata": {},
   "source": [
    "The following figure illustrates the average pairwise ARI as function of the number of trials. The average pairwise ARI is presented with a 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,3), dpi=100)\n",
    "xs = np.linspace(1,100,5)\n",
    "ax.plot(xs, avg_ARI, '-o', label=\"RAA\", color='#e3427d')\n",
    "ax.fill_between(xs,\n",
    "                 y1 = [x for (x,y) in CI_ARI],\n",
    "                 y2 = [y for (x,y) in CI_ARI],\n",
    "                 color='#e3427d', alpha=0.2)\n",
    "ax.plot(xs, [x for (x,y) in CI_ARI], '--', color='#e3427d')\n",
    "ax.plot(xs, [y for (x,y) in CI_ARI], '--', color='#e3427d')\n",
    "ax.set_xlabel(\"N trials\")\n",
    "ax.set_ylabel(\"ARI\")\n",
    "ax.set_title(\"Robustness of Ensemble Infomap\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41cb992",
   "metadata": {},
   "source": [
    "The figure demonstrates that the robustness of Ensemble Infomap significantly increases with the number of trials. Furthermore, the results also indicates that increasing the number of trials above 25 only leads to a marginal increase in the stability of the partitions in regards to this specific network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e06232",
   "metadata": {},
   "source": [
    "### Node Migration Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f74484e",
   "metadata": {},
   "source": [
    "The previous section demonstrated the usefulness of the proposed ensemble Infomap algorithm. Hence, the ensemble method will be used to conduct the independent community detections in the following creation of the node migration visualisation.\n",
    "\n",
    "The initial step in creating the visualisation is to compute the k-greatest communities at each time step. The following step is then to create edges between communities, for instance, at time step $t_i$ and $t_{i+1}$ if a node in a community at $t_i$ also is present in a community at $t_{i+1}$. To keep the visualisation manageable only a single edge will be created between a pair of communities. However, the edge weight will correspond to the number of nodes migrating from one community to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ba815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sankey(t0, tN, edgelist, k, N, threshold, verbose=False):\n",
    "\n",
    "    #Creating networkx graph objects for each year\n",
    "    networks = []\n",
    "    tot_communities = []\n",
    "    number_communities = {}\n",
    "    for year in [t0 + i for i in range(tN-t0 + 1)]:\n",
    "        edgelist_temp = edgelist[edgelist.timestamp==year]\n",
    "        attrs_temp = {row.source: {'group': row['source universe']} for idx, row in edgelist_temp.iterrows()}\n",
    "        G_temp = nx.from_pandas_edgelist(edgelist_temp, create_using = nx.DiGraph)\n",
    "\n",
    "        #Adding attributes\n",
    "        nx.set_node_attributes(G_temp, attrs_temp)\n",
    "        \n",
    "        #Fixing issue with nodes missing universe data\n",
    "        missing_attrs = {row.target: {'group': row['target universe']}for idx, row in edgelist_temp[edgelist_temp['target'].isin([u for u,v in G_temp.nodes(data=True) if not v])].iterrows()}\n",
    "        nx.set_node_attributes(G_temp, missing_attrs)\n",
    "\n",
    "\n",
    "        #Removing singleton nodes\n",
    "        G_temp.remove_nodes_from(list(nx.isolates(G_temp)))\n",
    "        \n",
    "        #Calling ensemble infomap to compute communities\n",
    "        communities = ensemble_infomap(graph=G_temp, N=N, k=k, threshold=threshold, verbose=verbose)\n",
    "        number_communities[year] = len(list(communities.keys()))\n",
    "        tot_communities.append(communities)\n",
    "        \n",
    "    edges = []\n",
    "    plot_names = []\n",
    "    for idx, (com_t0, com_tN) in enumerate(zip(tot_communities, tot_communities[1:])):\n",
    "        \n",
    "        if idx == 0:\n",
    "            prev_max = 0\n",
    "        \n",
    "        idx_map_t0 = dict(zip(list(com_t0.keys()),\n",
    "                           [i for i in range(prev_max, prev_max + len(list(com_t0.keys()))+1)]))\n",
    "        #updating idx for next iter\n",
    "        prev_max = max(idx_map_t0.values())+ 1\n",
    "\n",
    "        idx_map_tN = dict(zip(list(com_tN.keys()),\n",
    "                           [i for i in range(prev_max, prev_max + len(com_tN.keys())+1)]))\n",
    "        \n",
    "        for u,v in com_t0.items(): #t0\n",
    "            for node in v:\n",
    "                for i,j in com_tN.items(): #tN\n",
    "                    if node in j:\n",
    "                        edges.append((idx_map_t0[u],idx_map_tN[i]))\n",
    "\n",
    "                        break\n",
    "\n",
    "        plot_names.append(list(idx_map_t0.values()))\n",
    "    plot_names.append(list(idx_map_tN.values()))\n",
    "    plot_names = [item for sublist in plot_names for item in sublist]\n",
    "    \n",
    "    \n",
    "    source = []\n",
    "    target = []\n",
    "    value = []\n",
    "    for (u,v), w in dict(Counter(edges)).items():\n",
    "        source.append(u)\n",
    "        target.append(v)\n",
    "        value.append(w)\n",
    "\n",
    "    #Fixing plotly error\n",
    "    for plot_name in plot_names:\n",
    "        if plot_name not in target and plot_name not in source:\n",
    "            source.append(plot_name)\n",
    "            target.append(plot_name)\n",
    "            value.append(1)\n",
    "    \n",
    "    #Determining positions - has to be between 0-1\n",
    "    y = []\n",
    "    x = []\n",
    "    year_range = np.linspace(0, 1, len(number_communities.values()))\n",
    "    \n",
    "    \n",
    "    for idx, number in enumerate(number_communities.values()):\n",
    "        y.append(np.linspace(0, 1, number))\n",
    "        x.append([year_range[idx]]*number) \n",
    "        \n",
    "    x_pos = [item for sublist in x for item in sublist]\n",
    "    y_pos = [item for sublist in y for item in sublist]\n",
    "    \n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "            arrangement = \"snap\",\n",
    "            node = dict(\n",
    "                pad = 10,\n",
    "                thickness = 15,\n",
    "                line = dict(color = \"black\", width=0.5),\n",
    "                label = plot_names,\n",
    "                x = x_pos,\n",
    "                y = y_pos,\n",
    "                color = \"darkblue\"\n",
    "\n",
    "            ),\n",
    "            link = dict(\n",
    "                source = source,\n",
    "                target = target,\n",
    "                value = value,\n",
    "                color = [\n",
    "                        px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)]\n",
    "                        for i in source\n",
    "                       ]\n",
    "            )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(title_text=f\"Community Evolution {t0}-{tN}\", font_size=15, \n",
    "                      yaxis={'visible': False, 'showticklabels': False},\n",
    "                      xaxis={'visible': False, 'showticklabels': False})\n",
    "    \n",
    "    for x_coordinate, column_name in enumerate([f\"{t0+i}\" for i in range(tN-t0 + 1)]):\n",
    "        fig.add_annotation(\n",
    "              x=x_coordinate,#Plotly recognizes 0-5 to be the x range.\n",
    "\n",
    "              y=1.075,#y value above 1 means above all nodes\n",
    "              xref=\"x\",\n",
    "              yref=\"paper\",\n",
    "              text=column_name,#Text\n",
    "              showarrow=False,\n",
    "              font=dict(\n",
    "                  family=\"Times New Roman\",\n",
    "                  size=16,\n",
    "                  color=\"black\"\n",
    "                  ),\n",
    "              align=\"left\",\n",
    "              )\n",
    "\n",
    "    fig.add_annotation(text='Self-loops indicate no node migration to the previous or next K-greatest communities', \n",
    "                    align='left',\n",
    "                    showarrow=False,\n",
    "                    xref='paper',\n",
    "                    yref='paper',\n",
    "                    x=0,\n",
    "                    y=-0.1,\n",
    "                    bordercolor='black',\n",
    "                    borderwidth=1,\n",
    "                    font=dict(\n",
    "                          family=\"Times New Roman\",\n",
    "                          size=12,\n",
    "                          color=\"black\"\n",
    "                  ))\n",
    "    \n",
    "    return fig, tot_communities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fea4bd",
   "metadata": {},
   "source": [
    "The following visualisation demonstrate the node migration of the 10 greatest communities within the time-period 2015-2022. To both demonstrate the effect of ensemble Infomap and to keep the running time of the notebook convenient, the following demonstration will use the original Infomap without ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42666b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, temp = sankey(t0=2015, tN=2022, k=10, edgelist=expanded_edgelist, N=1, threshold=0.9, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a4f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5bc948",
   "metadata": {},
   "source": [
    "To visualise the node migration throughout the entire timeframe (2001-2022), the following figures demonstrate the intervals (2001-2008), (2008-2015), and (2015-2022). The figures are generated using the 10 greatest communities and an ensemble Infomap using 100 trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d404785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a23cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src='communitiy_evolve_sankey_2001_2008.html', width=1000, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src='communitiy_evolve_sankey_2008_2015.html', width=1000, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e519cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src='communitiy_evolve_sankey_2015_2022.html', width=1000, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e7e77",
   "metadata": {},
   "source": [
    "> **_Note:_** The colours of edges do not correspond to a specific community. They are added to make it easier to see which outgoing edges correspond to a given community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771bc86",
   "metadata": {},
   "source": [
    "Restricting the attention to the first illustration, which depicts the community evolution from 2001 to 2008, this period can be characterised by rapid community structure changes, with multiple examples of apparent growth, merging, splitting, birth, and death of communities. The only community evolution characteristic that does not seem to be present within this timeframe is the contraction of communities. Furthermore, some of the communities also appear to demonstrate relatively continuous growth from 2001 to 2008. A striking shift in the community structure seems to happen from 2006 to 2007, where the greatest community in 2006 splits into multiple communities in 2007. A further investigation of why and what rewiring of the network diagram causes this community behaviour would be interesting.\n",
    "\n",
    "The community evolution of the k-greatest communities from 2008 to 2015 is characterised by generally more stability, with fewer examples of splitting, merging etc., compared with the structural changes present from 2001 to 2008. Lastly, the community evolution from 2015 to 2022 also demonstrates generally stable clusters with a decrease in node migration between different communities compared with the previous timeframes.\n",
    "\n",
    "Thus, the depicted evolution demonstrates that the stability of the k-greatest communities increases with time. This finding aligns with the expectations based on the yearly number of revisions distribution of the Wikipedia pages previously illustrated, which demonstrated a significant decrease in the number of revisions after the year 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6f34e",
   "metadata": {},
   "source": [
    "### Robustness of Community Assignment\n",
    "As previously mentioned, the node migration visualisations indicated that the stability of k-greatest communities increased with time. Concerning this finding, One hypothesis to explain this trend could be as previously mentioned that the decreasing number of revisions causes this behaviour. Another hypothesis could be that due to the increasing number of edges each year, presented previously, the amount of rewiring in the network necessary to alter a node's community assignment increases proportionally. Hence, it would be interesting to investigate if this trend potentially could be explained by this second hypothesis. One possible approach to verify or falsify this hypothesis would be to examine the number of randomly added edges required to change the community assignment of a given node as a function of the number of edges in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7611cca",
   "metadata": {},
   "source": [
    "# Further Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbed53c",
   "metadata": {},
   "source": [
    "There are still things left to discover about this network;\n",
    "\n",
    "> - How is the metadata put together? Would it make sense to adjust the jaccard similarity to get more certain on the data we have?\n",
    "> - We need to do some statistical tests on how the network evolves. We can do this with permutation tests which should allow us to see if the network grows in a coral fashion at times.\n",
    "> - Why does the stability of the communities increase with time? Could it be because the number of revisions of the wikipedia pages decreases after 2007? Or could it be because the required rewiring of the network to reassign a node's communityy increases with the size of the network? Maybe a third option is the explanation for instance that the content of Wikipedia pages converges towards a collective agreement?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22981691",
   "metadata": {},
   "source": [
    "# Summary / Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad26e8",
   "metadata": {},
   "source": [
    "Let's conclude the distinct sections of this notebook;\n",
    "> - We saw that both the number of nodes, edges and the average degree of the network increased dramatically between 2004 and 2007. This correlates with the general trend of article revisions in [Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Statistics).\n",
    "> - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64429233",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Evkoski B, Mozetič I, Ljubešić N, Kralj Novak P (2021) Community evolution in retweet networks. PLOS ONE 16(9): e0256175.\n",
    "<br >\n",
    "[2] Heaberlin, Bradi, and Simon DeDeo. 2016. \"The Evolution of Wikipedia’s Norm Network\" Future Internet 8, no. 2: 14. https://doi.org/10.3390/fi8020014 \n",
    "<br >\n",
    "[3] Barabási, Albert-László. 2015. Network science. Cambridge University Press. http://networksciencebook.com/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
